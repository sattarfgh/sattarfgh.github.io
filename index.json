[{"body":"","link":"https://sattarfgh.github.io/","section":"","tags":null,"title":""},{"body":"","link":"https://sattarfgh.github.io/tags/airflow/","section":"tags","tags":null,"title":"Airflow"},{"body":"Once you’ve decided to use Apache Airflow in your production, it would be a great idea to take some time and plan for it. The first question that comes to mind is how it should be deployed in production. To answer this question you need to familiarize yourself with the airflow architecture. Airflow has 6 main components:\n Database: It primarily keeps all the states. Scheduler: This is responsible for parsing DAGs and scheduling them based on their schedule interval. Executor: It is responsible for running an instance of the DAG, if a local executor has been selected the DAGs run inside the scheduler, but if a remote executor has been chosen the DAGs deliver to the workers for running them. Worker: It is responsible for executing the scheduled DAGs when the remote executor is chosen. Webserver: it is simply a user interface that allows users to monitor and manage the DAGs. DAG directory: A directory where all the DAGs will be stored.  So based on the given architecture, we can concentrate on each piece.\n1. Replace the default database Given that the first component of the airflow is the database, let's get started with it. By default, airflow employs SQLite to keep the data and it makes sense to replace it with a production-grade database like MySQL or Postgresql.\n2. Time zone The Airflow Scheduler is responsible for scheduling the DAGs. The scheduling happens based on the default Airflow's time zone or the designated time zone defined in the DAGs and not the server's local time. So always make sure to set up the right time zone for your settings.\n3. Daemonize them! In production, both the scheduler and the webserver should be up and running. So, based on your preferred Linux distribution, you need to define those processes as a daemon. For instance, instead of running these commands!\n1# airflow webserver 2# airflow scheduler You might consider these daemons!\n1# systemctl start airflow-scheduler 2# systemctl start airflow-webserver 4. Executer matters The executor's type should be selected before deploying the airflow. Generally, we do have 2 types of executors: local and remote. The local basically means that the scheduler is in charge of running the DAG in the local machine, but the remote executor indicates that the DAG should be delivered to remote machines to be executed. Definitely, for production-grade deployment, the remote executor would be preferable!\n5. Security Broadly speaking, there are two major components in place that you need to secure them:\n Webserver: running a DAG or modifying the connections may have an adverse impact on your business. As such, it should be auditable who ran a dag or modified it. It is recommended that either you define users for each individual or connect to an LDAP service. Another concern would be the network communication between users and the webserver. So it seems essential to enable TLS over this communication. Database: The best way to store the connections in the apache airflow is to keep them in a Secrets Backend such as HashiCorp Vault, but if you want to keep them in the airflow's database it is recommended that encrypt them (Fernet key).  6. Keep the DAGs in a repository Basically, the DAGs are python codes, so like any other code, it should be kept in a control version system such as git. It helps you to track changes in the code files and allows better code review.\n7- Be stylish Always follow style conventions that make the DAGs cleaner and easier to understand, especially if you are working in a huge setting. For example, you can easily use pycodestyle and yapf to examine your coding style and its quality.\n8- Alerting Some of the DAGs are considered as mission critical, meaning their failure would hurt the business. That’s mandatory to have some mechanism in place in order to alert the designated individual to respond when it is broken. At DAG’s level, you can use on_failure_callback to define required actions when the DAG fails. And also it is vital checking the status of the scheduler itself and alert someone when it is down!\n","link":"https://sattarfgh.github.io/post/apache-airflow-best-practices/","section":"post","tags":["Airflow"],"title":"Apache Airflow Best Practices"},{"body":"","link":"https://sattarfgh.github.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://sattarfgh.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://sattarfgh.github.io/archives/","section":"","tags":null,"title":""},{"body":"","link":"https://sattarfgh.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://sattarfgh.github.io/series/","section":"series","tags":null,"title":"Series"}]